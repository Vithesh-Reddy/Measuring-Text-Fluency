{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from alive_progress import alive_bar\n",
    "import copy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_SIZE = 4\n",
    "ngramDicts = {}\n",
    "\n",
    "MODEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text and returns a list of tokens\n",
    "def get_token_list(text: str) -> list:\n",
    "    # lower case it\n",
    "    text = text.lower()\n",
    "    # tokenize hashtags\n",
    "    text = re.sub(r\"#(\\w+)\", r\"<HASHTAG> \", text)\n",
    "    text = re.sub(r'\\d+(,(\\d+))*(\\.(\\d+))?%?\\s', '<NUMBER> ', text)\n",
    "    # tokenize mentions\n",
    "    text = re.sub(r\"@(\\w+)\", r\"<MENTION> \", text)\n",
    "    # tokenize urls\n",
    "    text = re.sub(r\"http\\S+\", r\"<URL> \", text)\n",
    "    # starting with www\n",
    "    text = re.sub(r\"www\\S+\", r\"<URL> \", text)\n",
    "\n",
    "    special_chars = [' ', '*', '!', '?', '.', ',', ';', ':', '(', ')', '[', ']', '{', '}', '/', '\\\\', '|', '-', '_', 'â€”','=','+', '`', '~', '@', '#', '$', '%', '^', '&', '0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
    "    # pad the special characters with spaces\n",
    "    for char in special_chars:\n",
    "        text = text.replace(char, ' ')\n",
    "    # pad < and > with spaces\n",
    "    text = text.replace('<', ' <')\n",
    "    text = text.replace('>', '> ')\n",
    "\n",
    "    return text.split()\n",
    "# slipts the text into sentences and tokenizes them. \n",
    "def sentence_tokenizer(fullText: str, thresh: int) -> list:\n",
    "    # lower case it\n",
    "    fullText = fullText.lower()\n",
    "    # tokenize hashtags\n",
    "    fullText = re.sub(r\"#(\\w+)\", r\"<HASHTAG> \", fullText)\n",
    "    # tokenize mentions\n",
    "    fullText = re.sub(r\"@(\\w+)\", r\"<MENTION> \", fullText)\n",
    "    # tokenize urls\n",
    "    fullText = re.sub(r\"http\\S+\", r\"<URL> \", fullText)\n",
    "    # starting with www\n",
    "    fullText = re.sub(r\"www\\S+\", r\"<URL> \", fullText)\n",
    "    sentenceEnders = ['.', '!', '?']\n",
    "    # split on sentence enders handling cases such as Mr. etc\n",
    "    fullText = fullText.replace('mr.', 'mr')\n",
    "    fullText = fullText.replace('mrs.', 'mrs')\n",
    "    fullText = fullText.replace('dr.', 'dr')\n",
    "    fullText = fullText.replace('st.', 'st')\n",
    "    fullText = fullText.replace('co.', 'co')\n",
    "    fullText = fullText.replace('inc.', 'inc')\n",
    "    fullText = fullText.replace('e.g.', 'eg')\n",
    "    fullText = fullText.replace('i.e.', 'ie')\n",
    "    fullText = fullText.replace('etc.', 'etc')\n",
    "    fullText = fullText.replace('vs.', 'vs')\n",
    "    fullText = fullText.replace('u.s.', 'us')\n",
    "\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', fullText)\n",
    "\n",
    "    sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if s != '']\n",
    "    sentences = [get_token_list(s) for s in sentences]\n",
    "\n",
    "    tokenDict = {}\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token in tokenDict:\n",
    "                tokenDict[token] += 1\n",
    "            else:\n",
    "                tokenDict[token] = 1\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if tokenDict[sentence[i]] <= thresh:\n",
    "                sentence[i] = '<unk>'\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# replaces all tokens with frequency less than threshold with <unk>\n",
    "def rem_low_freq(tokens: list, threshold: int) -> list:\n",
    "    # get the frequency of each token\n",
    "    freq = {}\n",
    "    for token in tokens:\n",
    "        if token in freq:\n",
    "            freq[token] += 1\n",
    "        else:\n",
    "            freq[token] = 1\n",
    "\n",
    "    # remove tokens with frequency less than threshold\n",
    "    for token in list(freq.keys()):\n",
    "        if freq[token] <= threshold:\n",
    "            del freq[token]\n",
    "\n",
    "    # replace all tokens not in freq with <unk>\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in freq:\n",
    "            tokens[i] = '<unk>'\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# constructs an n-gram dictionary from the input token list\n",
    "def construct_ngram(n: int, token_list: list) -> dict:\n",
    "    ngram_dict = {}\n",
    "    for i in range(len(token_list) - n + 1):\n",
    "        ngram_to_check = token_list[i:i + n]\n",
    "        cur_dict = ngram_dict\n",
    "        for j in range(n):\n",
    "            if ngram_to_check[j] not in cur_dict:\n",
    "                if j == n - 1:\n",
    "                    cur_dict[ngram_to_check[j]] = 1\n",
    "                else:\n",
    "                    cur_dict[ngram_to_check[j]] = {}\n",
    "            else:\n",
    "                if j == n - 1:\n",
    "                    cur_dict[ngram_to_check[j]] += 1\n",
    "            cur_dict = cur_dict[ngram_to_check[j]]\n",
    "\n",
    "    return ngram_dict\n",
    "# counts the number of n-grams in the input n-gram dictionary using dfs\n",
    "def dfs_count(ngram_dict: dict) -> int:\n",
    "    count = 0\n",
    "    for key, value in ngram_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            count += dfs_count(value)\n",
    "        else:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# gives the count of the input n-gram\n",
    "def ngram_count(ngram_dict: dict, ngram: list) -> int:\n",
    "    cur_dict = ngram_dict[len(ngram)]\n",
    "    if len(ngram) == 1:\n",
    "        if ngram[0] in cur_dict:\n",
    "            return cur_dict[ngram[0]]\n",
    "        else:\n",
    "            return cur_dict['<unk>']\n",
    "    for i in range(len(ngram)):\n",
    "        if ngram[i] in cur_dict:\n",
    "            cur_dict = cur_dict[ngram[i]]\n",
    "        else:\n",
    "            return 0\n",
    "    return cur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_countD = {}\n",
    "\n",
    "def kneser_ney_smoothing(ngram_dict: dict, d: float, ngram: list) -> float:\n",
    "    # Replace unknown tokens in ngram with '<unk>'\n",
    "    ngram = ['<unk>' if token not in ngram_dict[1] else token.lower() for token in ngram]\n",
    "\n",
    "    if len(ngram) == 1:\n",
    "        # If unigram, compute probability directly\n",
    "        denom = dfs_countD.get(2, dfs_count(ngram_dict[2]))\n",
    "        count = sum(1 for value in ngram_dict[2].values() if ngram[-1] in value)\n",
    "        return count/denom\n",
    "\n",
    "    deno = ngram_count(ngram_dict, ngram[:-1])\n",
    "    if(deno == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        first = max(ngram_count(ngram_dict, ngram) - d, 0) / deno\n",
    "    \n",
    "    try:\n",
    "        cur_dict = ngram_dict[len(ngram)]\n",
    "        for token in ngram[:-1]:\n",
    "            cur_dict = cur_dict[token]\n",
    "        second_rhs = len(cur_dict)\n",
    "    except KeyError:\n",
    "        second_rhs = 0\n",
    "    \n",
    "    second = d * second_rhs / ngram_count(ngram_dict, ngram[:-1])\n",
    "    \n",
    "    return first + second * kneser_ney_smoothing(ngram_dict, d, ngram[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns witten bell smoothed probability of the input n-gram\n",
    "def witten_bell_smoothing(ngram_dict: dict, ngram: list) -> float:\n",
    "    # Replace unknown tokens in ngram with '<unk>'\n",
    "    ngram = ['<unk>' if token not in ngram_dict[1] else token.lower() for token in ngram]\n",
    "\n",
    "    if len(ngram) == 1:\n",
    "        return ngram_count(ngram_dict, ngram) / len(ngram_dict[1])\n",
    "\n",
    "    try:\n",
    "        cur_dict = ngram_dict[len(ngram)]\n",
    "        for token in ngram[:-1]:\n",
    "            cur_dict = cur_dict[token]\n",
    "        lambda_inv_num = len(cur_dict)\n",
    "    except KeyError:\n",
    "        lambda_inv_num = 0\n",
    "\n",
    "    deno = ngram_count(ngram_dict, ngram[:-1]) + lambda_inv_num\n",
    "    if(deno == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        lambda_val = lambda_inv_num / deno\n",
    "        first = lambda_val * ngram_count(ngram_dict, ngram) / ngram_count(ngram_dict, ngram[:-1])\n",
    "        second = (1 - lambda_val) * witten_bell_smoothing(ngram_dict, ngram[1:])\n",
    "        return first + second\n",
    "    # try:\n",
    "    #     lambda_inv_num /= lambda_inv_num + ngram_count(ngram_dict, ngram[:-1])\n",
    "    # except ZeroDivisionError:\n",
    "    #     return 0\n",
    "\n",
    "    # lambd = 1 - lambda_inv_num\n",
    "\n",
    "    # first_term = lambd * ngram_count(ngram_dict, ngram) / ngram_count(ngram_dict, ngram[:-1])\n",
    "    # second_term = lambda_inv_num * witten_bell_smoothing(ngram_dict, ngram[1:])\n",
    "\n",
    "    # return first_term + second_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the likelihood of the input sentence\n",
    "def sentence_likelihood(ngram_dict: dict, sentence: list, smoothing: str, kneserd=0.75) -> float:\n",
    "    tokens = sentence\n",
    "    if smoothing == 'w' or smoothing == 'wb':\n",
    "        likelihood = 0\n",
    "        for i in range(len(tokens) - NGRAM_SIZE + 1):\n",
    "            likelihood += np.log(max(witten_bell_smoothing(ngram_dict, tokens[i:i + NGRAM_SIZE]), 1e-15))\n",
    "        return likelihood\n",
    "    elif smoothing == 'k' or smoothing == 'kn':\n",
    "        likelihood = 0\n",
    "        for i in range(len(tokens) - NGRAM_SIZE + 1):\n",
    "            likelihood += np.log(max(kneser_ney_smoothing(ngram_dict, kneserd, tokens[i:i + NGRAM_SIZE]), 1e-15))\n",
    "        return likelihood\n",
    "\n",
    "# calculates the perplexity of the input sentence\n",
    "def perplexity(ngram_dict: dict, sentence: list, smoothing: str, kneserd=0.75) -> float:\n",
    "    prob = sentence_likelihood(ngram_dict, sentence, smoothing, kneserd)\n",
    "    prob = np.exp(prob)\n",
    "    prob = max(prob, 1e-15)\n",
    "    return pow(prob, -1 / len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(perp_list, file_name=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(perp_list, bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perp_plots(text, train):\n",
    "    sentences = copy.deepcopy(text)\n",
    "    sentences = sentence_tokenizer(sentences, 1)\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) >= NGRAM_SIZE]\n",
    "    Lines = sentences.copy()\n",
    "    if(train == True):\n",
    "        combined_text = \"\"\n",
    "        for line in Lines:\n",
    "            combined_text += \" \".join(line) + \" \"\n",
    "\n",
    "        tokens = rem_low_freq(get_token_list(combined_text), 1)\n",
    "\n",
    "        for n in range(NGRAM_SIZE):\n",
    "            ngramDicts[n + 1] = construct_ngram(n + 1, tokens)   \n",
    "\n",
    "    wb_perplexities = []\n",
    "    toWrite = []\n",
    "    with alive_bar(len(Lines)) as bar:\n",
    "        for sentence in Lines:\n",
    "            wb_perplexities.append(perplexity(ngramDicts, sentence, 'wb'))\n",
    "            toWrite.append(\" \".join(sentence) + \"\\t\" + str(wb_perplexities[-1]))\n",
    "            bar()\n",
    "    # Plotting\n",
    "    get_plot(wb_perplexities, 'wb_plot_all.png')\n",
    "\n",
    "    wb_avg = sum(wb_perplexities) / len(wb_perplexities)\n",
    "    wb_median = np.median(wb_perplexities)\n",
    "    print(f'Witten-Bell average perplexity: {wb_avg}')\n",
    "    print(f'Witten-Bell median perplexity: {wb_median}')\n",
    "    # write average perplexity at the top\n",
    "    outputfile = open(f\"LM{MODEL + 1}_train-perplexity.txt\", \"w\", encoding=\"utf-8\")\n",
    "    outputfile.write(f'{wb_avg}\\n')\n",
    "    outputfile.write(\"\\n\".join(toWrite))\n",
    "    outputfile.close()\n",
    "\n",
    "    # # print(\"strated\")\n",
    "    # kn_perplexities = []\n",
    "    # toWrite = []\n",
    "    # with alive_bar(len(Lines)) as bar:\n",
    "    #     for sentence in Lines:\n",
    "    #         kn_perplexities.append(perplexity(ngramDicts, sentence, 'kn'))\n",
    "    #         toWrite.append(\" \".join(sentence) + \"\\t\" + str(kn_perplexities[-1]))\n",
    "    #         bar()\n",
    "\n",
    "    # kn_avg = sum(kn_perplexities) / len(kn_perplexities)\n",
    "    # kn_median = np.median(kn_perplexities)\n",
    "    # outputfile = open(f\"LM{MODEL}_train-perplexity.txt\", \"w\", encoding=\"utf-8\")\n",
    "    # # write average perplexity at the top\n",
    "    # outputfile.write(f'{kn_avg}\\n')\n",
    "    # outputfile.write(\"\\n\".join(toWrite))\n",
    "    # outputfile.close()\n",
    "    # print(f'Kneser-Ney average perplexity: {kn_avg}')\n",
    "    # print(f'Kneser-Ney median perplexity: {kn_median}')\n",
    "    # # plotting\n",
    "    # get_plot(kn_perplexities, 'kn_plot_all.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCsv = csv.DictReader(open(\"C4_200M_1M.csv\", \"r\", encoding=\"utf-8\"))\n",
    "sentences = []\n",
    "testSentences = []\n",
    "for row in inputCsv:\n",
    "    sentences.append(row[\"output\"])\n",
    "    testSentences.append(row[\"input\"])\n",
    "sentences = sentences[:20000]\n",
    "testSentences = testSentences[-20000:]\n",
    "fullText = \" \".join(sentences)\n",
    "testText = \" \".join(test_sentences)\n",
    "# tokens = rem_low_freq(get_token_list(fullText), 1)\n",
    "# ngramDicts = {}\n",
    "# for n in range(NGRAM_SIZE):\n",
    "#     ngramDicts[n + 1] = construct_ngram(n + 1, tokens)\n",
    "# print(\"preparing..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perp_plots(fullText, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perp_plots(testText, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "inputCsv = csv.DictReader(open(\"C4_200M_1M.csv\", \"r\", encoding=\"utf-8\"))\n",
    "for row in inputCsv:\n",
    "    test_sentences.append(row[\"input\"])\n",
    "print(test_sentences)\n",
    "# consider last 20k sentences for testing\n",
    "test_sentences = test_sentences[-20000:]\n",
    "testText = \" \".join(test_sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perp_plots(testText, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
